{
  "llm_models": {
    "LLM": {
      "base_url": "https://api.openai.com/v1",
      "chat_endpoint": "/chat/completions",
      "api_key": "Bearer sk-llm",
      "concurrency_limit": 10,
      "timeout": 30
    },
    "OpenAI": {
      "base_url": "https://api.openai.com/v1",
      "chat_endpoint": "/chat/completions",
      "api_key": "Bearer sk-openai",
      "concurrency_limit": 10,
      "timeout": 30
    },
    "Other": {
      "base_url": "https://api.openai.com/v1",
      "chat_endpoint": "/chat/completions",
      "api_key": "Bearer sk-other",
      "concurrency_limit": 10,
      "timeout": 30
    },
    "Dify": {
      "base_url": "http://172.16.10.25/v1",
      "chat_endpoint": "/chat-messages",
      "conv_endpoint": "/conversations",
      "api_key": "Bearer app-dify",
      "conv_limit": 5,
      "sort_by": "-created_at",
      "concurrency_limit": 10,
      "timeout": 30
    },
    "FastGPT": {
      "base_url": "https://api.openai.com/v1",
      "chat_endpoint": "/chat/completions",
      "api_key": "Bearer sk-fastgpt",
      "concurrency_limit": 10,
      "timeout": 30
    }
  },
  "llm_param": {
    "LLM": {
      "model": "llm",
      "messages": [],
      "stream": false,
      "temperature": 0.7,
      "max_tokens": 1024,
      "user": "user-123",
      "query": "",
      "inputs": {},
      "conversation_id": ""
    },
    "OpenAI": {
      "model": "openai",
      "messages": [],
      "stream": false,
      "temperature": 0.7,
      "max_tokens": 1024,
      "user": "user-123",
      "query": "",
      "inputs": {},
      "conversation_id": ""
    },
    "Other": {
      "model": "other",
      "messages": [],
      "stream": false,
      "temperature": 0.7,
      "max_tokens": 1024,
      "user": "user-123",
      "query": "",
      "inputs": {},
      "conversation_id": ""
    },
    "Dify": {
      "model": "dify",
      "query": "",
      "response_mode": "streaming",
      "user": "user-123",
      "conversation_id": "",
      "inputs": {},
      "messages": [],
      "stream": true
    },
    "FastGPT": {
      "model": "fastgpt",
      "messages": [],
      "stream": false,
      "temperature": 0.7,
      "max_tokens": 1024,
      "user": "user-123",
      "query": "",
      "inputs": {},
      "conversation_id": ""
    }
  }
}
